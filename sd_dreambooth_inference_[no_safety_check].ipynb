{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abesmon/dreambooth-remix/blob/main/sd_dreambooth_inference_%5Bno_safety_check%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Dreambooth fine-tuned models for Stable Diffusion using düß®ffusers \n",
        "\n",
        "This notebook allows you to run Stable Diffusion concepts trained via Dreambooth using ü§ó Hugging Face [üß® Diffusers library](https://github.com/huggingface/diffusers). \n",
        "\n",
        "Train your own using [here](#) and navigate the [public library concepts](#) to pick yours. You may also want to use the [Spaces](#) to browse the library\n",
        "\n",
        "\n",
        "![Dreambooth Example](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n",
        "_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_ \n",
        "\n",
        "Differently from Textual Inversion, this approach trains the whole model, which can yield better results to the cost of bigger models.\n",
        "\n"
      ],
      "metadata": {
        "id": "0tb1ywtV_EEO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KFOytKQG-gGY"
      },
      "outputs": [],
      "source": [
        "#@title Install and import requirements\n",
        "!pip install -qqq diffusers==0.4.1 transformers gradio ftfy \n",
        "\n",
        "import diffusers\n",
        "import gradio\n",
        "from PIL import Image\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to the Hugging Face Hub\n",
        "#@markdown Optional step, do it if you want to run private concepts\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cG56KJga44EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the model from the [Concepts Library](https://huggingface.co/sd-dreambooth-library). If you are new to Stable Diffusion, make sure you [read the LICENSE](https://github.com/CompVis/stable-diffusion/blob/main/LICENSE)\n",
        "#@markdown  You may also use a locally trained model by replacing the `model_id` to a path with the model locally or on Google Drive\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\" #@param {type:\"string\"}\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def monkeypatch_fwd(clip_input, images):\n",
        "  return images, False\n",
        "\n",
        "pipe.safety_checker.forward = monkeypatch_fwd"
      ],
      "metadata": {
        "id": "ZntPH3F6-myj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the Stable Diffusion pipeline with interactive UI Demo on Gradio\n",
        "#@markdown Run this cell to get a Gradio UI like this to run your models\n",
        "\n",
        "#@markdown ![](https://i.imgur.com/bxHfawQ.png)\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, num_samples):\n",
        "    all_images = [] \n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
        "    all_images.extend(images)\n",
        "    return all_images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(\"<h2 style=\\\"font-size: 2em; font-weight: bold\\\" align=\\\"center\\\">Stable Diffusion Dreambooth - Run Concept</h2>\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"prompt\")\n",
        "            samples = gr.Slider(label=\"Samples\",value=1)\n",
        "            run = gr.Button(value=\"Run\")\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery(show_label=False)\n",
        "\n",
        "    run.click(inference, inputs=[prompt,samples], outputs=gallery)\n",
        "    gr.Examples([[\"a photo of sks toy riding a bicycle\", 1,1]], [prompt,samples], gallery, inference, cache_examples=False)\n",
        "\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sUE8A7znAlvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the Stable Diffusion pipeline on Colab\n",
        "#@markdown Don't forget to use the `sks` token in your prompt\n",
        "\n",
        "from torch import autocast\n",
        "prompt = \"a photo of sks toy floating on a ramen bowl\" #@param {type:\"string\"}\n",
        "\n",
        "num_samples = 1 #@param {type:\"number\"}\n",
        "num_rows = 2 #@param {type:\"number\"}\n",
        "\n",
        "all_images = [] \n",
        "for _ in range(num_rows):\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
        "    all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, num_samples, num_rows)\n",
        "grid"
      ],
      "metadata": {
        "id": "DXBVVJjl_lwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title –ó–∞–ø—É—Å–∫ SD —Å–æ —Å—Ç–∞—Ç–∏—á–Ω—ã–º —Å–∏–¥–æ–º\n",
        "\n",
        "from torch import autocast\n",
        "prompt = \"a photo of sks toy floating on a ramen bowl\" #@param {type:\"string\"}\n",
        "seed = 1234 #@param {type:\"number\"}\n",
        "num_samples = 1 #@param {type:\"number\"}\n",
        "num_rows = 2 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "num_inference_steps=50 #@param {type:\"number\"}\n",
        "guidance_scale=7.5 #@param {type:\"number\"}\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "all_images = [] \n",
        "for _ in range(num_rows):\n",
        "    images = pipe(prompt, width=width, height=height, num_images_per_prompt=num_samples, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator).images\n",
        "    all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, num_samples, num_rows)\n",
        "grid"
      ],
      "metadata": {
        "id": "MNTwPZRKlK8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}